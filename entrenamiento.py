"""
===============================================================================
TALLER DE MACHINE LEARNING: CLASIFICACIÃ“N DE BILLETES FALSOS
Dataset: Banknote Authentication (UCI ML Repository)
Autor: [Tu Nombre]
InstituciÃ³n: PolitÃ©cnico Colombiano Jaime Isaza Cadavid
Fecha: 2024
===============================================================================

Este script implementa:
1. Carga y exploraciÃ³n del dataset
2. Preprocesamiento de datos
3. Entrenamiento de RegresiÃ³n LogÃ­stica
4. Entrenamiento de Red Neuronal Artificial
5. EvaluaciÃ³n y comparaciÃ³n de modelos
6. GeneraciÃ³n de todas las figuras para el paper
7. Guardado de modelos entrenados
"""

# ============================================================================
# IMPORTAR LIBRERÃAS
# ============================================================================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    confusion_matrix, 
    accuracy_score, 
    precision_score,
    recall_score, 
    f1_score, 
    classification_report,
    roc_curve,
    auc
)
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import pickle
import json
import warnings
warnings.filterwarnings('ignore')

# ConfiguraciÃ³n de visualizaciÃ³n
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")
plt.rcParams['figure.dpi'] = 150
plt.rcParams['font.size'] = 10

print("="*80)
print("TALLER: CLASIFICACIÃ“N DE BILLETES BANCARIOS AUTÃ‰NTICOS VS FALSOS")
print("="*80)
print("\nâœ… LibrerÃ­as importadas correctamente\n")

# ============================================================================
# 1. CARGAR Y EXPLORAR EL DATASET
# ============================================================================
print("="*80)
print("PASO 1: CARGA Y EXPLORACIÃ“N DEL DATASET")
print("="*80)

# URL del dataset en UCI Repository
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt"
column_names = ['variance', 'skewness', 'curtosis', 'entropy', 'class']

try:
    # Intentar cargar desde UCI
    df = pd.read_csv(url, names=column_names)
    print("âœ… Dataset cargado exitosamente desde UCI Repository\n")
except Exception as e:
    print(f"âš ï¸ No se pudo descargar: {e}")
    print("Creando dataset sintÃ©tico para demostraciÃ³n...\n")
    
    # Dataset sintÃ©tico si no hay internet
    np.random.seed(42)
    n = 1372
    df = pd.DataFrame({
        'variance': np.random.randn(n) * 2.5 + 0.5,
        'skewness': np.random.randn(n) * 3.5,
        'curtosis': np.random.randn(n) * 3,
        'entropy': np.random.randn(n) * 1.8 - 0.3,
        'class': np.random.randint(0, 2, n)
    })

# InformaciÃ³n bÃ¡sica del dataset
print("ğŸ“Š INFORMACIÃ“N DEL DATASET:")
print(f"   â€¢ Total de muestras: {len(df)}")
print(f"   â€¢ NÃºmero de caracterÃ­sticas: {len(df.columns) - 1}")
print(f"   â€¢ Billetes autÃ©nticos (clase 0): {len(df[df['class'] == 0])} ({len(df[df['class'] == 0])/len(df)*100:.1f}%)")
print(f"   â€¢ Billetes falsos (clase 1): {len(df[df['class'] == 1])} ({len(df[df['class'] == 1])/len(df)*100:.1f}%)")

print("\nğŸ“‹ Primeras 5 filas del dataset:")
print(df.head())

print("\nğŸ“ˆ EstadÃ­sticas descriptivas:")
print(df.describe())

print("\nâ“ Valores nulos por columna:")
print(df.isnull().sum())

print("\nâœ… ConclusiÃ³n: Dataset limpio, sin valores nulos")

# ============================================================================
# 2. ANÃLISIS EXPLORATORIO DE DATOS (EDA)
# ============================================================================
print("\n" + "="*80)
print("PASO 2: ANÃLISIS EXPLORATORIO DE DATOS")
print("="*80)

# Crear figura completa de EDA
fig = plt.figure(figsize=(16, 12))
gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

# TÃ­tulo principal
fig.suptitle('AnÃ¡lisis Exploratorio: Dataset de Billetes Bancarios', 
             fontsize=16, fontweight='bold', y=0.995)

# 1. DistribuciÃ³n de clases
ax1 = fig.add_subplot(gs[0, 0])
class_counts = df['class'].value_counts()
colors_bar = ['#2ecc71', '#e74c3c']
bars = ax1.bar(['AutÃ©ntico (0)', 'Falso (1)'], class_counts.values, 
               color=colors_bar, alpha=0.8, edgecolor='black')
ax1.set_title('DistribuciÃ³n de Clases', fontweight='bold')
ax1.set_ylabel('Cantidad de Muestras')
for bar in bars:
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height,
             f'{int(height)}\n({height/len(df)*100:.1f}%)',
             ha='center', va='bottom', fontweight='bold')

# 2-5. Distribuciones de caracterÃ­sticas
features = ['variance', 'skewness', 'curtosis', 'entropy']
colors = ['#3498db', '#e74c3c']
positions = [(0, 1), (0, 2), (1, 0), (1, 1)]

for idx, (feature, pos) in enumerate(zip(features, positions)):
    ax = fig.add_subplot(gs[pos[0], pos[1]])
    
    # Histogramas superpuestos
    for class_label, color in zip([0, 1], colors):
        data = df[df['class'] == class_label][feature]
        ax.hist(data, bins=30, alpha=0.6, color=color, 
                label=f'Clase {class_label}', edgecolor='black', linewidth=0.5)
    
    ax.set_title(f'DistribuciÃ³n: {feature.capitalize()}', fontweight='bold')
    ax.set_xlabel(feature.capitalize())
    ax.set_ylabel('Frecuencia')
    ax.legend()
    ax.grid(True, alpha=0.3)

# 6. Matriz de correlaciÃ³n
ax6 = fig.add_subplot(gs[1, 2])
corr_matrix = df.corr()
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
            center=0, square=True, ax=ax6, cbar_kws={'shrink': 0.8})
ax6.set_title('Matriz de CorrelaciÃ³n', fontweight='bold')

# 7. Boxplot de Varianza
ax7 = fig.add_subplot(gs[2, 0])
df.boxplot(column='variance', by='class', ax=ax7, patch_artist=True,
           boxprops=dict(facecolor='lightblue', alpha=0.7))
ax7.set_title('Varianza por Clase', fontweight='bold')
ax7.set_xlabel('Clase')
ax7.set_ylabel('Varianza')
plt.sca(ax7)
plt.xticks([1, 2], ['AutÃ©ntico', 'Falso'])

# 8. Boxplot de Curtosis
ax8 = fig.add_subplot(gs[2, 1])
df.boxplot(column='curtosis', by='class', ax=ax8, patch_artist=True,
           boxprops=dict(facecolor='lightcoral', alpha=0.7))
ax8.set_title('Curtosis por Clase', fontweight='bold')
ax8.set_xlabel('Clase')
ax8.set_ylabel('Curtosis')
plt.sca(ax8)
plt.xticks([1, 2], ['AutÃ©ntico', 'Falso'])

# 9. Pairplot simplificado (scatter de las 2 mejores caracterÃ­sticas)
ax9 = fig.add_subplot(gs[2, 2])
for class_label, color, label in zip([0, 1], colors, ['AutÃ©ntico', 'Falso']):
    data = df[df['class'] == class_label]
    ax9.scatter(data['variance'], data['curtosis'], 
                c=color, alpha=0.6, s=20, label=label, edgecolors='black', linewidth=0.3)
ax9.set_title('Varianza vs Curtosis', fontweight='bold')
ax9.set_xlabel('Varianza')
ax9.set_ylabel('Curtosis')
ax9.legend()
ax9.grid(True, alpha=0.3)

plt.savefig('EDA_completo.png', dpi=300, bbox_inches='tight')
print("\nâœ… Figura guardada: EDA_completo.png")
plt.close()

# ============================================================================
# 3. PREPARACIÃ“N DE DATOS
# ============================================================================
print("\n" + "="*80)
print("PASO 3: PREPARACIÃ“N DE DATOS")
print("="*80)

# Separar caracterÃ­sticas (X) y etiquetas (y)
X = df.drop('class', axis=1).values
y = df['class'].values

print(f"âœ… Forma de X (caracterÃ­sticas): {X.shape}")
print(f"âœ… Forma de y (etiquetas): {y.shape}")

# DivisiÃ³n entrenamiento/prueba (70%/30%) con estratificaciÃ³n
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

print(f"\nğŸ“¦ Conjunto de entrenamiento: {X_train.shape[0]} muestras ({X_train.shape[0]/len(X)*100:.1f}%)")
print(f"   - AutÃ©nticos: {np.sum(y_train == 0)}")
print(f"   - Falsos: {np.sum(y_train == 1)}")

print(f"\nğŸ“¦ Conjunto de prueba: {X_test.shape[0]} muestras ({X_test.shape[0]/len(X)*100:.1f}%)")
print(f"   - AutÃ©nticos: {np.sum(y_test == 0)}")
print(f"   - Falsos: {np.sum(y_test == 1)}")

# NormalizaciÃ³n con StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\nâœ… Datos normalizados con StandardScaler (media=0, std=1)")
print(f"   Media de X_train_scaled: {X_train_scaled.mean(axis=0).round(4)}")
print(f"   Std de X_train_scaled: {X_train_scaled.std(axis=0).round(4)}")

# Guardar el scaler para uso futuro
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)
print("\nğŸ’¾ Scaler guardado: scaler.pkl")

# ============================================================================
# 4. MODELO 1: REGRESIÃ“N LOGÃSTICA
# ============================================================================
print("\n" + "="*80)
print("MODELO 1: REGRESIÃ“N LOGÃSTICA")
print("="*80)

# Entrenar modelo
print("\nâš™ï¸ Entrenando RegresiÃ³n LogÃ­stica...")
lr_model = LogisticRegression(
    max_iter=1000,
    random_state=42,
    solver='lbfgs',
    C=1.0  # ParÃ¡metro de regularizaciÃ³n
)

lr_model.fit(X_train_scaled, y_train)
print("âœ… Modelo entrenado exitosamente")

# InformaciÃ³n del modelo
print("\nğŸ“Š INFORMACIÃ“N DEL MODELO:")
print(f"   â€¢ Coeficientes (pesos):")
for feature, coef in zip(column_names[:-1], lr_model.coef_[0]):
    print(f"     - {feature}: {coef:.4f}")
print(f"   â€¢ Intercepto (bias): {lr_model.intercept_[0]:.4f}")

# Predicciones
y_pred_lr = lr_model.predict(X_test_scaled)
y_pred_lr_proba = lr_model.predict_proba(X_test_scaled)[:, 1]

# Calcular mÃ©tricas
cm_lr = confusion_matrix(y_test, y_pred_lr)
accuracy_lr = accuracy_score(y_test, y_pred_lr)
precision_lr = precision_score(y_test, y_pred_lr)
recall_lr = recall_score(y_test, y_pred_lr)
f1_lr = f1_score(y_test, y_pred_lr)
error_lr = 1 - accuracy_lr

print("\nğŸ“Š RESULTADOS - REGRESIÃ“N LOGÃSTICA:")
print("="*60)
print(f"   Error:          {error_lr:.4f} ({error_lr*100:.2f}%)")
print(f"   Exactitud:      {accuracy_lr:.4f} ({accuracy_lr*100:.2f}%)")
print(f"   PrecisiÃ³n:      {precision_lr:.4f} ({precision_lr*100:.2f}%)")
print(f"   Exhaustividad:  {recall_lr:.4f} ({recall_lr*100:.2f}%)")
print(f"   F1-Score:       {f1_lr:.4f} ({f1_lr*100:.2f}%)")

print("\nğŸ“‹ Matriz de ConfusiÃ³n:")
print(f"                Predicho: 0    Predicho: 1")
print(f"Real: 0 (Auth)      {cm_lr[0,0]:3d}           {cm_lr[0,1]:3d}")
print(f"Real: 1 (Fake)      {cm_lr[1,0]:3d}           {cm_lr[1,1]:3d}")

print("\nğŸ” InterpretaciÃ³n:")
print(f"   â€¢ Verdaderos Negativos (TN): {cm_lr[0,0]} - AutÃ©nticos correctamente identificados")
print(f"   â€¢ Falsos Positivos (FP): {cm_lr[0,1]} - AutÃ©nticos clasificados como falsos")
print(f"   â€¢ Falsos Negativos (FN): {cm_lr[1,0]} - Falsos clasificados como autÃ©nticos âš ï¸")
print(f"   â€¢ Verdaderos Positivos (TP): {cm_lr[1,1]} - Falsos correctamente identificados")

print(f"\nğŸ“ˆ Reporte de clasificaciÃ³n completo:")
print(classification_report(y_test, y_pred_lr, target_names=['AutÃ©ntico', 'Falso']))

# Guardar modelo
with open('logistic_regression_model.pkl', 'wb') as f:
    pickle.dump(lr_model, f)
print("ğŸ’¾ Modelo guardado: logistic_regression_model.pkl")

# ============================================================================
# 5. MODELO 2: RED NEURONAL ARTIFICIAL
# ============================================================================
print("\n" + "="*80)
print("MODELO 2: RED NEURONAL ARTIFICIAL")
print("="*80)

# Construir arquitectura
print("\nğŸ—ï¸ Construyendo arquitectura de la red neuronal...")
nn_model = Sequential([
    Dense(16, activation='relu', input_shape=(X_train_scaled.shape[1],), 
          name='hidden_layer_1'),
    Dropout(0.2, name='dropout_1'),
    Dense(8, activation='relu', name='hidden_layer_2'),
    Dropout(0.2, name='dropout_2'),
    Dense(1, activation='sigmoid', name='output_layer')
], name='BanknoteClassifier')

# Compilar modelo
nn_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='binary_crossentropy',
    metrics=['accuracy', 'Precision', 'Recall']
)

print("\nğŸ“ ARQUITECTURA DE LA RED NEURONAL:")
nn_model.summary()

# Callbacks para entrenamiento
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=15,
    restore_best_weights=True,
    verbose=1
)

checkpoint = ModelCheckpoint(
    'best_nn_model.h5',
    monitor='val_accuracy',
    save_best_only=True,
    verbose=0
)

# Entrenar modelo
print("\nâš™ï¸ Entrenando Red Neuronal (esto puede tardar 1-2 minutos)...")
history = nn_model.fit(
    X_train_scaled, y_train,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stopping, checkpoint],
    verbose=0
)

print(f"âœ… Entrenamiento completado en {len(history.history['loss'])} Ã©pocas")

# Predicciones
y_pred_nn_proba = nn_model.predict(X_test_scaled, verbose=0).flatten()
y_pred_nn = (y_pred_nn_proba > 0.5).astype(int)

# Calcular mÃ©tricas
cm_nn = confusion_matrix(y_test, y_pred_nn)
accuracy_nn = accuracy_score(y_test, y_pred_nn)
precision_nn = precision_score(y_test, y_pred_nn)
recall_nn = recall_score(y_test, y_pred_nn)
f1_nn = f1_score(y_test, y_pred_nn)
error_nn = 1 - accuracy_nn

print("\nğŸ“Š RESULTADOS - RED NEURONAL:")
print("="*60)
print(f"   Error:          {error_nn:.4f} ({error_nn*100:.2f}%)")
print(f"   Exactitud:      {accuracy_nn:.4f} ({accuracy_nn*100:.2f}%)")
print(f"   PrecisiÃ³n:      {precision_nn:.4f} ({precision_nn*100:.2f}%)")
print(f"   Exhaustividad:  {recall_nn:.4f} ({recall_nn*100:.2f}%)")
print(f"   F1-Score:       {f1_nn:.4f} ({f1_nn*100:.2f}%)")

print("\nğŸ“‹ Matriz de ConfusiÃ³n:")
print(f"                Predicho: 0    Predicho: 1")
print(f"Real: 0 (Auth)      {cm_nn[0,0]:3d}           {cm_nn[0,1]:3d}")
print(f"Real: 1 (Fake)      {cm_nn[1,0]:3d}           {cm_nn[1,1]:3d}")

print("\nğŸ” InterpretaciÃ³n:")
print(f"   â€¢ Verdaderos Negativos (TN): {cm_nn[0,0]}")
print(f"   â€¢ Falsos Positivos (FP): {cm_nn[0,1]}")
print(f"   â€¢ Falsos Negativos (FN): {cm_nn[1,0]} âš ï¸")
print(f"   â€¢ Verdaderos Positivos (TP): {cm_nn[1,1]}")

print(f"\nğŸ“ˆ Reporte de clasificaciÃ³n completo:")
print(classification_report(y_test, y_pred_nn, target_names=['AutÃ©ntico', 'Falso']))

# Guardar modelo
nn_model.save('neural_network_model.h5')
print("ğŸ’¾ Modelo guardado: neural_network_model.h5")

# Guardar historial de entrenamiento
with open('training_history.json', 'w') as f:
    json.dump(history.history, f)
print("ğŸ’¾ Historial guardado: training_history.json")

# ============================================================================
# 6. COMPARACIÃ“N DE MODELOS
# ============================================================================
print("\n" + "="*80)
print("COMPARACIÃ“N DE MODELOS")
print("="*80)

comparison_df = pd.DataFrame({
    'MÃ©trica': ['Error', 'Exactitud', 'PrecisiÃ³n', 'Exhaustividad', 'F1-Score'],
    'RegresiÃ³n LogÃ­stica': [
        f'{error_lr:.4f}',
        f'{accuracy_lr:.4f}',
        f'{precision_lr:.4f}',
        f'{recall_lr:.4f}',
        f'{f1_lr:.4f}'
    ],
    'Red Neuronal': [
        f'{error_nn:.4f}',
        f'{accuracy_nn:.4f}',
        f'{precision_nn:.4f}',
        f'{recall_nn:.4f}',
        f'{f1_nn:.4f}'
    ],
    'Diferencia': [
        f'{error_nn - error_lr:+.4f}',
        f'{accuracy_nn - accuracy_lr:+.4f}',
        f'{precision_nn - precision_lr:+.4f}',
        f'{recall_nn - recall_lr:+.4f}',
        f'{f1_nn - f1_lr:+.4f}'
    ]
})

print("\n" + comparison_df.to_string(index=False))

# Determinar ganador
if accuracy_nn > accuracy_lr:
    winner = "Red Neuronal"
    diff = (accuracy_nn - accuracy_lr) * 100
    print(f"\nğŸ† GANADOR: {winner}")
    print(f"   Mejora en exactitud: +{diff:.2f}%")
    print(f"   ReducciÃ³n de errores: {((error_lr - error_nn) / error_lr * 100):.1f}%")
elif accuracy_lr > accuracy_nn:
    winner = "RegresiÃ³n LogÃ­stica"
    diff = (accuracy_lr - accuracy_nn) * 100
    print(f"\nğŸ† GANADOR: {winner}")
    print(f"   Mejora en exactitud: +{diff:.2f}%")
else:
    print(f"\nğŸ¤ EMPATE: Ambos modelos tienen exactitud similar")

# ============================================================================
# 7. GUARDAR RESULTADOS FINALES
# ============================================================================
print("\n" + "="*80)
print("GUARDANDO RESULTADOS FINALES")
print("="*80)

results = {
    'dataset_info': {
        'total_samples': len(df),
        'authentic': int(np.sum(y == 0)),
        'fake': int(np.sum(y == 1)),
        'train_size': len(X_train),
        'test_size': len(X_test)
    },
    'logistic_regression': {
        'error': float(error_lr),
        'accuracy': float(accuracy_lr),
        'precision': float(precision_lr),
        'recall': float(recall_lr),
        'f1_score': float(f1_lr),
        'confusion_matrix': cm_lr.tolist()
    },
    'neural_network': {
        'error': float(error_nn),
        'accuracy': float(accuracy_nn),
        'precision': float(precision_nn),
        'recall': float(recall_nn),
        'f1_score': float(f1_nn),
        'confusion_matrix': cm_nn.tolist(),
        'epochs_trained': len(history.history['loss'])
    }
}

with open('resultados_finales.json', 'w') as f:
    json.dump(results, f, indent=4)

print("ğŸ’¾ Resultados guardados: resultados_finales.json")

print("\n" + "="*80)
print("âœ… ENTRENAMIENTO COMPLETADO EXITOSAMENTE")
print("="*80)
print("\nArchivos generados:")
print("   ğŸ“Š EDA_completo.png")
print("   ğŸ”µ logistic_regression_model.pkl")
print("   ğŸ”´ neural_network_model.h5")
print("   ğŸ”´ best_nn_model.h5")
print("   ğŸ“ˆ training_history.json")
print("   ğŸ“Š resultados_finales.json")
print("   ğŸ”§ scaler.pkl")
print("\n" + "="*80)